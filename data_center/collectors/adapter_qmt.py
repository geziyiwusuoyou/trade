# -*- coding: utf-8 -*-
"""
Module: adapter_qmt.py
Description: QMT æ•°æ®æºé€‚é…å™¨ - æ”¯æŒå…¨é‡/å¢é‡æ›´æ–°ï¼Œç²¾ç¡®æ¶¨è·Œåœè®¡ç®—
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from decimal import Decimal, ROUND_HALF_UP

# å¼•å…¥ QMT SDK
from xtquant import xtdata

# å¼•å…¥é…ç½®
from config import MARKET_DATA_DIR
from common.data_structs import BarFields

# ================= 1. æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def _round_to_2_decimals(number):
    """Aè‚¡ä»·æ ¼ä¸“ç”¨å››èˆäº”å…¥"""
    d = Decimal(str(number))
    return float(d.quantize(Decimal("0.00"), rounding=ROUND_HALF_UP))

def _calculate_limit_price(code: str, name: str, prev_close: float):
    """è®¡ç®—æ¶¨è·Œåœä»·æ ¼ (åŒå‰æ–‡é€»è¾‘)"""
    if prev_close is None or np.isnan(prev_close):
        return np.nan, np.nan
    
    limit_ratio = 0.10
    if code.startswith('688') or code.startswith('30'):
        limit_ratio = 0.20
    elif code.startswith('8') or code.startswith('4'):
        limit_ratio = 0.30
    elif 'ST' in name:
        limit_ratio = 0.05
    
    up_price = _round_to_2_decimals(prev_close * (1 + limit_ratio))
    down_price = _round_to_2_decimals(prev_close * (1 - limit_ratio))
    return up_price, down_price

# ================= 2. QMT æ•°æ®åŠ è½½å™¨ç±» =================

class QMTDataLoader:
    def __init__(self):
        self.save_dir = MARKET_DATA_DIR / "stock_daily"
        if not self.save_dir.exists():
            self.save_dir.mkdir(parents=True, exist_ok=True)

    def _get_local_last_date(self, file_path):
        """
        è·å–æœ¬åœ° Parquet æ–‡ä»¶çš„æœ€åä¸€æ¡æ•°æ®æ—¥æœŸ
        :return: datetimeå¯¹è±¡ æˆ– None (å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨)
        """
        if not file_path.exists():
            return None
        try:
            # ä¼˜åŒ–ï¼šåªè¯»å–ç´¢å¼•åˆ—ï¼Œé€Ÿåº¦æå¿«
            # engine='pyarrow' é€šå¸¸æ¯” fastparquet å¿«ï¼Œè§†ç¯å¢ƒè€Œå®š
            df = pd.read_parquet(file_path, columns=[BarFields.DATE_TIME])
            if df.empty:
                return None
            return df.index[-1] # å‡è®¾ç´¢å¼•æ˜¯ datetime
        except Exception:
            return None

    def fetch_and_update(self, stock_list, start_str, end_str, mode="append"):
        """
        é€šç”¨æ ¸å¿ƒæ–¹æ³•ï¼šä¸‹è½½ -> æ¸…æ´— -> å­˜å‚¨
        :param stock_list: è‚¡ç¥¨ä»£ç åˆ—è¡¨
        :param start_str: '20230101'
        :param end_str: '20230105'
        :param mode: 'overwrite' (è¦†ç›–/å…¨é‡) or 'append' (å¢é‡)
        """
        print(f"ğŸ“¥ [QMT] æ­£åœ¨ä¸‹è½½æ•°æ®: {start_str} ~ {end_str}, æ¨¡å¼: {mode}, æ•°é‡: {len(stock_list)}")
        
        # 1. è§¦å‘ä¸‹è½½ (Blocking)
        for i, code in enumerate(stock_list):
            xtdata.download_history_data(code, period='1d', start_time=start_str, end_time=end_str)
            if (i + 1) % 500 == 0:
                print(f"   ä¸‹è½½è¿›åº¦: {i + 1}/{len(stock_list)}")

        # 2. æ‰¹é‡è·å–
        qmt_fields = ['time', 'open', 'high', 'low', 'close', 'volume', 'amount']
        data_dict = xtdata.get_market_data_ex(
            field_list=qmt_fields,
            stock_list=stock_list,
            period='1d',
            start_time=start_str,
            end_time=end_str,
            fill_data=True
        )

        success_count = 0
        
        # 3. é€ä¸ªå¤„ç†
        for code, new_df in data_dict.items():
            if new_df.empty: continue

            try:
                # --- æ¸…æ´— ---
                new_df['time'] = pd.to_datetime(new_df['time'], unit='ms') + pd.Timedelta(hours=8)
                new_df.rename(columns={
                    'time': BarFields.DATE_TIME, 'open': BarFields.OPEN,
                    'high': BarFields.HIGH, 'low': BarFields.LOW,
                    'close': BarFields.CLOSE, 'volume': BarFields.VOLUME,
                    'amount': BarFields.AMOUNT
                }, inplace=True)
                new_df.set_index(BarFields.DATE_TIME, inplace=True)
                new_df.sort_index(inplace=True)
                new_df[BarFields.CODE] = code
                new_df[BarFields.ADJ_FACTOR] = 1.0

                file_path = self.save_dir / f"{code}.parquet"

                # --- æ¨¡å¼å¤„ç† ---
                if mode == "append" and file_path.exists():
                    # è¯»å–æ—§æ•°æ®
                    old_df = pd.read_parquet(file_path)
                    # åˆå¹¶ (concat) å¹¶å»é‡ (drop_duplicates)
                    # keep='last' ä¿è¯å¦‚æœæ—¥æœŸé‡å ï¼Œä»¥æœ€æ–°ä¸‹è½½çš„ä¸ºå‡†
                    combined_df = pd.concat([old_df, new_df])
                    combined_df = combined_df[~combined_df.index.duplicated(keep='last')]
                    combined_df.sort_index(inplace=True)
                    target_df = combined_df
                else:
                    # è¦†ç›–æ¨¡å¼ / æ–‡ä»¶ä¸å­˜åœ¨
                    target_df = new_df

                # --- æ¶¨è·Œåœè®¡ç®— (åœ¨æœ€ç»ˆ Merged çš„ DF ä¸Šè®¡ç®—ï¼Œä¿è¯æ˜¨æ”¶è¿ç»­æ€§) ---
                # æ³¨æ„ï¼šå¦‚æœæ˜¯ Append æ¨¡å¼ï¼Œæœ€å¥½åªé‡æ–°è®¡ç®—æ–°åŠ éƒ¨åˆ†çš„æ¶¨è·Œåœï¼Œ
                # ä½†ä¸ºäº†ä»£ç ç®€å•ä¸”é˜²æ­¢æ˜¨æ”¶ä¿®æ­£ï¼Œè¿™é‡Œå¯¹ target_df åšä¸€æ¬¡å…¨é‡è®¡ç®—ä¹Ÿå¾ˆå¿«ã€‚
                # ä¼˜åŒ–ç‚¹ï¼šå¦‚æœ target_df å¾ˆå¤§ï¼Œè¿™é‡Œå¯ä»¥ä¼˜åŒ–ã€‚
                
                target_df['prev_close'] = target_df[BarFields.CLOSE].shift(1)
                
                # è·å–åç§°
                instr_detail = xtdata.get_instrument_detail(code)
                stock_name = instr_detail['InstrumentName'] if instr_detail else ""

                # å‘é‡åŒ–è®¡ç®—æœ‰ç‚¹éš¾ï¼Œè¿˜æ˜¯ç”¨åˆ—è¡¨æ¨å¯¼
                limit_ups = []
                limit_downs = []
                
                # è¿™é‡Œä¸ºäº†æ•ˆç‡ï¼Œå¦‚æœæ•°æ®é‡ > 5000ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–ã€‚ç›®å‰æ—¥çº¿çº§åˆ«è¿˜å¥½ã€‚
                # å°æŠ€å·§ï¼šåªè®¡ç®—æœ€å N è¡Œï¼Ÿä¸ï¼Œä¸ºäº†æ•°æ®ä¸€è‡´æ€§ï¼Œå»ºè®®å…¨ç®—æˆ–åªç®—æ–°è¡Œã€‚
                # è¿™é‡Œæ¼”ç¤ºå…¨ç®—ï¼Œä¿è¯ä¸­é—´æ²¡æœ‰æ–­å±‚
                for idx, row in target_df.iterrows():
                    p_close = row['prev_close']
                    if pd.isna(p_close):
                        limit_ups.append(np.nan)
                        limit_downs.append(np.nan)
                    else:
                        u, d = _calculate_limit_price(code, stock_name, p_close)
                        limit_ups.append(u)
                        limit_downs.append(d)
                
                target_df[BarFields.LIMIT_UP] = limit_ups
                target_df[BarFields.LIMIT_DOWN] = limit_downs
                target_df.drop(columns=['prev_close'], inplace=True)

                # --- è½åº“ ---
                target_df.to_parquet(file_path)
                success_count += 1

            except Exception as e:
                print(f"âŒ å¤„ç† {code} å¤±è´¥: {e}")
                continue
        
        print(f"âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œæ›´æ–°äº† {success_count} ä¸ªæ–‡ä»¶ã€‚")

    # ================= A. å…¨é‡æ›´æ–°æ¨¡å¼ =================
    def run_full_update(self, start_date: str, end_date: str):
        """
        å¼ºåˆ¶æŒ‡å®šæ—¥æœŸèŒƒå›´è¿›è¡Œå…¨é‡è¦†ç›–
        :param start_date: '20200101'
        :param end_date: '20231231'
        """
        print(f"ğŸš€ [æ¨¡å¼: å…¨é‡æ›´æ–°] èŒƒå›´: {start_date} ~ {end_date}")
        stock_list = xtdata.get_stock_list_in_sector('æ²ªæ·±Aè‚¡')
        # å…¨é‡æ¨¡å¼ç›´æ¥è°ƒç”¨é€šç”¨æ–¹æ³•ï¼Œæ¨¡å¼ä¸º overwrite
        self.fetch_and_update(stock_list, start_date, end_date, mode="overwrite")

    # ================= B. å¢é‡æ›´æ–°æ¨¡å¼ =================
    def run_incremental_update(self):
        """
        è‡ªåŠ¨æ£€æµ‹æ¯åªè‚¡ç¥¨çš„è¿›åº¦ï¼Œåªä¸‹è½½ç¼ºå¤±éƒ¨åˆ†
        """
        print(f"ğŸš€ [æ¨¡å¼: å¢é‡æ›´æ–°] æ­£åœ¨æ£€æŸ¥æœ¬åœ°æ•°æ®çŠ¶æ€...")
        stock_list = xtdata.get_stock_list_in_sector('æ²ªæ·±Aè‚¡')
        
        today = datetime.now()
        today_str = today.strftime('%Y%m%d')
        
        # å¾…æ›´æ–°åˆ—è¡¨ï¼šå­˜æ”¾ (code, start_date_str)
        # ä¸ºäº†é¿å…å¯¹æ¯åªè‚¡ç¥¨å‘ä¸€æ¬¡ download è¯·æ±‚ (å¤ªæ…¢)ï¼Œæˆ‘ä»¬å°†ç›¸åŒå¼€å§‹æ—¶é—´çš„è‚¡ç¥¨åˆ†ç»„
        update_groups = {} # { '20231027': ['000001', '000002'], ... }

        for code in stock_list:
            file_path = self.save_dir / f"{code}.parquet"
            last_dt = self._get_local_last_date(file_path)

            if last_dt is None:
                # æƒ…å†µ1: æ–°è‚¡æˆ–æœ¬åœ°æ— æ–‡ä»¶ -> é»˜è®¤ä¸‹è½½æœ€è¿‘365å¤© (æˆ–è€…ä½ å¯ä»¥è®¾ä¸ºä¸Šå¸‚æ—¥æœŸ)
                start_date = (today - timedelta(days=5)).strftime('%Y%m%d')
            else:
                # æƒ…å†µ2: æœ‰æ•°æ® -> æ£€æŸ¥æ˜¯å¦æ˜¯æœ€æ–°çš„
                # ç®€å•åˆ¤æ–­: å¦‚æœ last_dt æ˜¯ä»Šå¤©(ä¸”æ”¶ç›˜å)æˆ–æ˜¨å¤©ï¼Œå¯èƒ½ä¸éœ€è¦æ›´æ–°
                # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬æ€»æ˜¯å°è¯•è¯·æ±‚ last_dt ä¹‹åçš„æ•°æ®
                
                # å¦‚æœ last_dt å°±æ˜¯ä»Šå¤©ï¼Œä¸”ç°åœ¨æ˜¯ç›˜åï¼Œé‚£ä¸éœ€è¦æ›´æ–°
                # è¿™é‡Œç®€å•å¤„ç†ï¼šè¯·æ±‚ last_dt çš„ä¸‹ä¸€å¤©
                # æ³¨æ„ï¼šQMTå¦‚æœè¯·æ±‚çš„ start_time > end_timeï¼Œä¸ä¼šæŠ¥é”™ï¼Œåªä¼šè¿”å›ç©ºï¼Œè¿™å¾ˆå¥½
                next_day = last_dt + timedelta(days=1)
                if next_day > today:
                    continue # å·²ç»æ˜¯æœ€æ–°ï¼Œè·³è¿‡
                
                start_date = next_day.strftime('%Y%m%d')

            # åŠ å…¥åˆ†ç»„
            if start_date not in update_groups:
                update_groups[start_date] = []
            update_groups[start_date].append(code)

        # å¼€å§‹åˆ†ç»„ä¸‹è½½
        if not update_groups:
            print("âœ¨ æ‰€æœ‰æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return

        print(f"ğŸ“‹ æ£€æµ‹å®Œæ¯•ï¼Œå°†åˆ†ä¸º {len(update_groups)} ä¸ªæ—¶é—´æ‰¹æ¬¡è¿›è¡Œæ›´æ–°...")
        
        for start_str, codes in update_groups.items():
            # è¿‡æ»¤ä¸€ä¸‹ï¼Œå¦‚æœ start_str å·²ç»è¶…è¿‡äº† today_str (ç†è®ºä¸Šä¸Šé¢æ‹¦æˆªäº†)ï¼Œè·³è¿‡
            if start_str > today_str: continue
            
            print(f"   >> æ‰¹æ¬¡ {start_str} ~ {today_str}: åŒ…å« {len(codes)} åªè‚¡ç¥¨")
            # å¢é‡æ¨¡å¼ï¼Œä½¿ç”¨ append
            self.fetch_and_update(codes, start_str, today_str, mode="append")

if __name__ == "__main__":
    loader = QMTDataLoader()
    # # æµ‹è¯•å¢é‡
    # loader.run_incremental_update()
    # æµ‹è¯•å…¨é‡
    loader.run_full_update('201701', '20251205')