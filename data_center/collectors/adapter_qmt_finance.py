# -*- coding: utf-8 -*-
"""
Module: adapter_qmt_finance.py
Description: QMT è´¢åŠ¡æ•°æ®é€‚é…å™¨ (ä¿®å¤ç‰ˆ)
ä¿®å¤æ ¸å¿ƒBUG: QMTè¿”å›çš„DFç´¢å¼•æ˜¯RangeIndexï¼Œéœ€æ‰‹åŠ¨å°† m_anntime åˆ—è½¬ä¸ºç´¢å¼•
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import time

# QMT SDK
from xtquant import xtdata

# é…ç½®
from config import DATA_ROOT

class QMTFinanceLoader:
    def __init__(self):
        # å­˜å‚¨è·¯å¾„
        self.save_dir = DATA_ROOT / "financial_data"
        if not self.save_dir.exists():
            self.save_dir.mkdir(parents=True, exist_ok=True)
        
        self.tables = ['Balance', 'Income', 'CashFlow']
        self.DOWNLOAD_BATCH_SIZE = 50 

    def _get_local_last_ann_date(self, code):
        """è·å–æœ¬åœ° Parquet çš„æœ€æ–°å…¬å‘Šæ—¥"""
        file_path = self.save_dir / f"{code}.parquet"
        if not file_path.exists():
            return None
        try:
            # ä»…è¯»å–ç´¢å¼•
            df = pd.read_parquet(file_path, columns=[]) 
            if df.empty: return None
            return df.index.max()
        except:
            return None

    def fetch_and_update(self, stock_list, start_date=None, end_date=None, mode="append"):
        total_stocks = len(stock_list)
        print(f"ğŸ’° [Finance] å¯åŠ¨ä¿®å¤ç‰ˆæ›´æ–° | ç›®æ ‡: {total_stocks} åª")
        
        # ç›®æ ‡ç»“æŸæ—¶é—´
        target_end_str = end_date if end_date else datetime.now().strftime('%Y%m%d')
        target_end_dt = pd.to_datetime(target_end_str)

        # --- 1. æ™ºèƒ½ç­›é€‰ï¼šå“ªäº›éœ€è¦ä¸‹è½½ ---
        print("ğŸ” [é˜¶æ®µä¸€] æ‰«ææœ¬åœ°æ–‡ä»¶ï¼Œè®¡ç®—éœ€ä¸‹è½½åˆ—è¡¨...")
        stocks_to_download = []
        
        for code in stock_list:
            last_dt = self._get_local_last_ann_date(code)
            
            # åˆ¤è¯»é€»è¾‘
            if mode == "overwrite":
                stocks_to_download.append(code)
            elif last_dt is None:
                stocks_to_download.append(code)
            elif last_dt < (target_end_dt - timedelta(days=5)): 
                stocks_to_download.append(code)
            else:
                pass # è·³è¿‡

        print(f"ğŸ“‹ éœ€ä¸‹è½½/æ›´æ–°: {len(stocks_to_download)} (è·³è¿‡ {total_stocks - len(stocks_to_download)})")

        # --- 2. æ‰§è¡Œä¸‹è½½ (å¦‚æœéœ€è¦) ---
        if stocks_to_download:
            dl_start = start_date
            if not dl_start:
                # é»˜è®¤æœ€è¿‘ 3 å¹´ (å¢é‡æ¨¡å¼)
                dl_start = (datetime.now() - timedelta(days=365*3)).strftime('%Y%m%d')
            
            print(f"â¬‡ï¸ [é˜¶æ®µäºŒ] å¼€å§‹ä¸‹è½½ {len(stocks_to_download)} åªè‚¡ç¥¨ (èŒƒå›´: {dl_start}~{target_end_str})...")
            
            # åˆ†æ‰¹ä¸‹è½½
            for i in range(0, len(stocks_to_download), self.DOWNLOAD_BATCH_SIZE):
                batch = stocks_to_download[i : i + self.DOWNLOAD_BATCH_SIZE]
                try:
                    xtdata.download_financial_data2(
                        stock_list=batch,
                        table_list=self.tables,
                        start_time=dl_start,
                        end_time=target_end_str,
                        callback=lambda x: None
                    )
                except Exception as e:
                    print(f"   âš ï¸ ä¸‹è½½å¼‚å¸¸: {e}")
                
                if (i + 1) % 500 == 0:
                    print(f"   ...å·²å‘é€ä¸‹è½½è¯·æ±‚ {i + 1}/{len(stocks_to_download)}")
            
            print("âœ… ä¸‹è½½æŒ‡ä»¤å‘é€å®Œæˆï¼Œç­‰å¾…åå°åŒæ­¥...")
            # ç¨å¾®ç»™ç‚¹æ—¶é—´è®©QMTç¼“å­˜å†™å…¥ç£ç›˜ï¼Œé˜²æ­¢é©¬ä¸Šè¯»è¯»ä¸åˆ°
            time.sleep(2) 
        else:
            print("âœ… æœ¬åœ°æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡ä¸‹è½½")

        # --- 3. å¤„ç†ä¸è½åº“ (æ ¸å¿ƒä¿®å¤éƒ¨åˆ†) ---
        print("ğŸ”„ [é˜¶æ®µä¸‰] å¼€å§‹æ¸…æ´—ä¸è½åº“...")
        
        # åªè¦åœ¨è¿™ä¸ªåˆ—è¡¨é‡Œçš„ï¼Œæˆ–è€…æ˜¯å…¨é‡æ¨¡å¼ï¼Œéƒ½é‡æ–°å¤„ç†ä¸€éè½åº“
        target_list = stocks_to_download if mode == "append" else stock_list
        success_count = 0
        
        for i, code in enumerate(target_list):
            try:
                # A. è¯»å–åŸå§‹æ•°æ®
                data_map = xtdata.get_financial_data(
                    stock_list=[code], 
                    table_list=self.tables, 
                    start_time='', end_time='', 
                    report_type='announce_time'
                )
                
                dfs_to_merge = []
                
                # B. é€è¡¨æ¸…æ´—ç´¢å¼• (CRITICAL FIX)
                for tbl in self.tables:
                    df = data_map.get(tbl)
                    if df is None or df.empty: continue
                    
                    # === æ ¸å¿ƒä¿®å¤é€»è¾‘ ===
                    # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ 'm_anntime' åˆ— (å…¬å‘Šæ—¥)
                    if 'm_anntime' not in df.columns:
                        continue
                        
                    # 2. æ¸…æ´—å…¬å‘Šæ—¥ (è½¬å­—ç¬¦ä¸² -> è½¬datetime)
                    # å¤„ç† NaN æˆ– 0
                    df = df[df['m_anntime'].notna()] 
                    df = df[df['m_anntime'] != 0]
                    
                    if df.empty: continue
                    
                    # 3. è®¾ç½®ç´¢å¼•
                    # copy() é˜²æ­¢ SettingWithCopyWarning
                    df_clean = df.copy()
                    df_clean['ann_date'] = pd.to_datetime(df_clean['m_anntime'].astype(str), format='%Y%m%d', errors='coerce')
                    
                    # åˆ é™¤è½¬æ¢å¤±è´¥çš„æ—¥æœŸ (NaT)
                    df_clean = df_clean.dropna(subset=['ann_date'])
                    
                    # è®¾ä¸º Index
                    df_clean.set_index('ann_date', inplace=True)
                    df_clean.sort_index(inplace=True)
                    
                    # 4. ç»™åˆ—ååŠ åç¼€ (é˜²æ­¢ duplicate columns error)
                    # m_timetag æ˜¯å…±æœ‰çš„ï¼Œå¯ä»¥ä¿ç•™ä¸€ä¸ªæˆ–è€…éƒ½åŠ åç¼€
                    # è¿™é‡Œé€‰æ‹©åŠ åç¼€ï¼Œæ–¹ä¾¿åˆ†è¾¨
                    df_clean.columns = [f"{col}_{tbl}" if col != 'm_timetag' else col for col in df_clean.columns]
                    
                    dfs_to_merge.append(df_clean)

                if not dfs_to_merge:
                    continue

                # C. åˆå¹¶ (Outer Join)
                # ä½¿ç”¨ concat axis=1 è¿›è¡Œå¤–è¿æ¥åˆå¹¶
                # é‡åˆ°é‡å¤çš„ç´¢å¼•(åŒä¸€å¤©å‘äº†å¤šæ¬¡æŠ¥è¡¨?) -> drop_duplicates
                
                # å…ˆå»é‡æ¯ä¸ª DF çš„ç´¢å¼• (ç†è®ºä¸ŠåŒä¸€å¤©ä¸è¯¥æœ‰ä¸¤æ¡ï¼Œé™¤éä¿®æ­£)
                dfs_unique = []
                for d in dfs_to_merge:
                    dfs_unique.append(d[~d.index.duplicated(keep='last')])

                merged_df = pd.concat(dfs_unique, axis=1, join='outer')
                
                # åˆå¹¶åå¯èƒ½ä¼šæœ‰å¤šä¸ª m_timetag åˆ— (m_timetag, m_timetag, ...)
                # æˆ‘ä»¬å¯ä»¥åšä¸€ä¸ªæ•´ç†ï¼Œæˆ–è€…æš‚æ—¶ä¿ç•™

                # D. æ—¶é—´è¿‡æ»¤
                if start_date:
                    merged_df = merged_df[merged_df.index >= pd.Timestamp(start_date)]
                
                if merged_df.empty:
                    continue

                # E. å­˜å‚¨
                file_path = self.save_dir / f"{code}.parquet"
                
                if mode == "append" and file_path.exists():
                    try:
                        old_df = pd.read_parquet(file_path)
                        final_df = pd.concat([old_df, merged_df])
                        # å»é‡ï¼šæŒ‰ç´¢å¼•(å…¬å‘Šæ—¥)ï¼Œä¿ç•™æœ€æ–°çš„
                        final_df = final_df[~final_df.index.duplicated(keep='last')]
                        final_df.sort_index(inplace=True)
                    except:
                        final_df = merged_df
                else:
                    final_df = merged_df

                final_df.to_parquet(file_path)
                success_count += 1

            except Exception as e:
                # print(f"âŒ {code} å¼‚å¸¸: {e}")
                continue
            
            if (i + 1) % 200 == 0:
                print(f"   å·²å¤„ç†: {i + 1}/{len(target_list)} | æˆåŠŸ: {success_count}")

        print(f"\nâœ… [Finance] ä»»åŠ¡ç»“æŸ")
        print(f"   å°è¯•å¤„ç†: {len(target_list)}")
        print(f"   æˆåŠŸè½åº“: {success_count}")

    # ================= æ¥å£ =================
    def run_full_update(self, start_date, end_date):
        stock_list = xtdata.get_stock_list_in_sector('æ²ªæ·±Aè‚¡')
        self.fetch_and_update(stock_list, start_date, end_date, mode="overwrite")

    def run_incremental_update(self):
        stock_list = xtdata.get_stock_list_in_sector('æ²ªæ·±Aè‚¡')
        self.fetch_and_update(stock_list, mode="append")

if __name__ == "__main__":
    loader = QMTFinanceLoader()
    # è°ƒè¯•ï¼šåªè·‘20ä¸ª
    print(">>> Debugæ¨¡å¼ï¼šæµ‹è¯•å‰20ä¸ªè‚¡ç¥¨")
    test_list = xtdata.get_stock_list_in_sector('æ²ªæ·±Aè‚¡')[:20]
    # å¼ºåˆ¶è¦†ç›–æ¨¡å¼ï¼Œç¡®ä¿èƒ½çœ‹åˆ°å†™å…¥
    loader.fetch_and_update(test_list, start_date='20100101', mode="overwrite")